{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1913658,"sourceType":"datasetVersion","datasetId":1036526}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.160Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"self.encoder = tf.keras.Sequential([\n            layers.Input(shape=(150, 150, 3)),  # Grayscale Input\n            layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n            layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n            layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)\n        ])\n\n        # Decoder: Reconstructs a color image\n        self.decoder = tf.keras.Sequential([\n        layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2),\n        layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2),\n        layers.Conv2DTranspose(16, (3, 3), activation='relu', padding='same', strides=2),\n        \n        # Last layer should output 3 channels (RGB)\n        layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same'),  # 3 channels for color output\n        \n        # Crop extra pixels to match (150,150,3)\n        layers.Cropping2D(((1, 1), (1, 1)))  ","metadata":{}},{"cell_type":"markdown","source":"Data Loading","metadata":{}},{"cell_type":"markdown","source":"swish layer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\nclass AutoEncoder(Model):\n    def __init__(self, latent_dim, input_shape):\n        super(AutoEncoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.shape = input_shape\n\n        # Encoder: Extracts features from the input image\n        self.encoder = tf.keras.Sequential([\n            layers.Input(shape=(150, 150, 3)),  # Input layer\n            \n            layers.Conv2D(32, (3, 3), activation='swish', padding='same', strides=2),\n            layers.LayerNormalization(),\n            \n            layers.Conv2D(64, (3, 3), activation='swish', padding='same', strides=2),\n            layers.LayerNormalization(),\n            \n            layers.Conv2D(128, (3, 3), activation='swish', padding='same', strides=2),\n            layers.LayerNormalization()\n        ])\n\n        # Decoder: Reconstructs the image\n        self.decoder = tf.keras.Sequential([\n            layers.Conv2DTranspose(64, (3, 3), activation='swish', padding='same', strides=2),\n            layers.LayerNormalization(),\n            \n            layers.Conv2DTranspose(32, (3, 3), activation='swish', padding='same', strides=2),\n            layers.LayerNormalization(),\n            \n            layers.Conv2DTranspose(16, (3, 3), activation='swish', padding='same', strides=2),\n            layers.LayerNormalization(),\n            \n            # Last layer should output 3 channels (RGB)\n            layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same'),  # 3 channels for color output\n            \n            # Crop extra pixels to match (150,150,3)\n            layers.Cropping2D(((1, 1), (1, 1)))  \n        ])\n        \n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Set paths for input and output image folders\ninput_folder = '/kaggle/input/landscape-image-colorization/landscape Images/gray'\noutput_folder = '/kaggle/input/landscape-image-colorization/landscape Images/color'\n\n# Set paths for train, validation, and test directories\ntrain_input_folder = '/kaggle/working/to/train_input'\ntrain_output_folder = '/kaggle/working/to/train_output'\nval_input_folder = '/kaggle/working/to/val_input'\nval_output_folder = '/kaggle/working/to/val_output'\ntest_input_folder = '/kaggle/working/to/test_input'\ntest_output_folder = '/kaggle/working/to/test_output'\n\n# Create necessary directories if they don't e\n\n# Create necessary directories if they don't exist\nos.makedirs(train_input_folder, exist_ok=True)\nos.makedirs(train_output_folder, exist_ok=True)\nos.makedirs(val_input_folder, exist_ok=True)\nos.makedirs(val_output_folder, exist_ok=True)\nos.makedirs(test_input_folder, exist_ok=True)\nos.makedirs(test_output_folder, exist_ok=True)\n\n# List all images in the input folder (assuming both input and output images have the same filenames)\ninput_images = sorted(os.listdir(input_folder))\noutput_images = sorted(os.listdir(output_folder))\n\n# Ensure that input and output images are paired correctly (same names)\nassert input_images == output_images, \"Input and output images do not match!\"\n\n# Shuffle the image pairs\nimage_pairs = list(zip(input_images, output_images))\nrandom.shuffle(image_pairs)\n\n# Split the data (70% train, 20% validation, 10% test)\ntrain_size = int(len(image_pairs) * 0.7)\nval_size = int(len(image_pairs) * 0.2)\n\ntrain_pairs = image_pairs[:train_size]\nval_pairs = image_pairs[train_size:train_size + val_size]\ntest_pairs = image_pairs[train_size + val_size:]\n\n# Move files to the respective folders\nfor input_img, output_img in train_pairs:\n    shutil.copy(os.path.join(input_folder, input_img), os.path.join(train_input_folder, input_img))\n    shutil.copy(os.path.join(output_folder, output_img), os.path.join(train_output_folder, output_img))\n\nfor input_img, output_img in val_pairs:\n    shutil.copy(os.path.join(input_folder, input_img), os.path.join(val_input_folder, input_img))\n    shutil.copy(os.path.join(output_folder, output_img), os.path.join(val_output_folder, output_img))\n\nfor input_img, output_img in test_pairs:\n    shutil.copy(os.path.join(input_folder, input_img), os.path.join(test_input_folder, input_img))\n    shutil.copy(os.path.join(output_folder, output_img), os.path.join(test_output_folder, output_img))\n\nprint(f\"Training data size: {len(train_pairs)} images\")\nprint(f\"Validation data size: {len(val_pairs)} images\")\nprint(f\"Test data size: {len(test_pairs)} images\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\n\n# Load an image\nimage = cv2.imread(\"/kaggle/input/landscape-image-colorization/landscape Images/gray/0.jpg\")\n\n# Get the shape\nheight, width, channels = image.shape\nprint(f\"Image Shape: Height={height}, Width={width}, Channels={channels}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_images(folder, img_size=(150, 150)):\n    images = []\n    for filename in sorted(os.listdir(folder)):  \n        img_path = os.path.join(folder, filename)\n        \n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n        img = cv2.resize(img, img_size)  # Resize to (150,150)\n        \n        # Convert grayscale to 3-channel image\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n\n        images.append(img)\n\n    return np.array(images, dtype=np.float32) / 255.0\n\ndef load_imagesc(folder, img_size=(150, 150)):  \n    images = []\n    for filename in sorted(os.listdir(folder)):  \n        img_path = os.path.join(folder, filename)\n        \n        # Load the image in color mode\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)  \n        if img is None:\n            continue  # Skip if the image cannot be loaded\n        \n        # Resize to (150,150)\n        img = cv2.resize(img, img_size)  \n\n        # Convert BGR to RGB (OpenCV loads in BGR format)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        images.append(img)\n\n    # Convert to numpy array and normalize pixel values to [0,1]\n    return np.array(images, dtype=np.float32) / 255.0  \n\n# Set paths to train input/output images\n\n# Load images\nX_train = load_images(train_input_folder)\nY_train = load_imagesc(train_output_folder)\n\nX_val = load_images(val_input_folder)\nY_val = load_imagesc(val_output_folder)\n\nprint(f\"Training data shape: {X_train.shape}, {Y_train.shape}\")\nprint(f\"Validation data shape: {X_val.shape}, {Y_val.shape}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train[0].shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(X_train[0])  # Display the image\nplt.axis(\"off\")  # Hide axis\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nplt.imshow(Y_train[0])  # Display the image\nplt.axis(\"off\")  # Hide axis\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow.keras.backend as K\nimport tensorflow.image as tf_img\n\n# Function for PSNR metric\ndef psnr_metric(y_true, y_pred):\n    return tf.reduce_mean(tf_img.psnr(y_true, y_pred, max_val=1.0))\n\n# Function for SSIM metric\ndef ssim_metric(y_true, y_pred):\n    return tf.reduce_mean(tf_img.ssim(y_true, y_pred, max_val=1.0))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# swish","metadata":{}},{"cell_type":"code","source":"# Define input shape and latent space size\nimage_shape = (150, 150, 3)\nlatent_dim = 128  \n\nautoencoder = AutoEncoder(latent_dim=128,input_shape=image_shape)\n\nautoencoder.compile(optimizer='adam',loss='mse',metrics=[psnr_metric, ssim_metric])\n\n# Train the model\nhistory = autoencoder.fit(\n    X_train, Y_train, \n    validation_data=(X_val, Y_val),\n    epochs=70,  \n    batch_size=50,\n)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract loss values from history\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Plot the loss curves\nplt.figure(figsize=(8, 5))\nplt.plot(train_loss, label='Training Loss', color='blue')\nplt.plot(val_loss, label='Validation Loss', color='red')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test images\nX_test = load_images(test_input_folder)\n\n# Get reconstructed images\nreconstructed_images = autoencoder.predict(X_test)\n\n# Display original vs. reconstructed images\nimport matplotlib.pyplot as plt\n\nn = 5  # Number of images to visualize\nplt.figure(figsize=(10, 4))\nfor i in range(n):\n    plt.subplot(2, n, i + 1)\n    plt.imshow(X_test[i].reshape(150, 150,3))\n    plt.axis('off')\n    \n    plt.subplot(2, n, i + 1 + n)\n    plt.imshow(reconstructed_images[i].reshape(150, 150,3))\n    plt.axis('off')\n\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_imgs = autoencoder.encoder(X_test).numpy()\ndecoded_imgs = autoencoder.decoder(encoded_imgs).numpy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n  # display original\n  ax = plt.subplot(2, n, i + 1)\n  plt.imshow(X_test[i])\n  plt.title(\"original\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\n\n  # display reconstruction\n  ax = plt.subplot(2, n, i + 1 + n)\n  plt.imshow(decoded_imgs[i])\n  plt.title(\"reconstructed\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"Y_test = load_imagesc(test_input_folder)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After training, you can evaluate the model:\ntest_loss, test_psnr, test_ssim = autoencoder.evaluate(X_test, Y_test)  # Use your test data\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test PSNR: {test_psnr}\")\nprint(f\"Test SSIM: {test_ssim}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow.image as tf_img\nimport numpy as np\n\n# Get predictions on test data\npredictions = autoencoder.predict(X_test)\n\n# Calculate MSE manually\nmse = np.mean(np.square(Y_test - predictions))\nprint(f\"Test MSE: {mse}\")\n\n# Calculate PSNR (Peak Signal-to-Noise Ratio) manually\npsnr = np.mean(tf_img.psnr(Y_test, predictions, max_val=1.0))\nprint(f\"Test PSNR: {psnr}\")\n\n# Calculate SSIM (Structural Similarity Index) manually\nssim = np.mean(tf_img.ssim(Y_test, predictions, max_val=1.0))\nprint(f\"Test SSIM: {ssim}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Mean Squared Error (MSE)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Show the original and reconstructed images\nplt.figure(figsize=(10, 5))\n\n# Original image\nplt.subplot(1, 2, 1)\nplt.imshow(Y_test[0])  # Display original image\nplt.title(\"Original Image\")\nplt.axis(\"off\")\n\n# Reconstructed image\nplt.subplot(1, 2, 2)\nplt.imshow(predictions[0])  # Display reconstructed image\nplt.title(\"Reconstructed Image\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Interpretation:\n\n    MSE quantifies the average squared difference between the original and the reconstructed image. A lower MSE means the reconstruction is closer to the original image.\n    Scale: A smaller value indicates better reconstruction. Ideally, you want MSE to be as low as possible, but this varies depending on the image data and model architecture.","metadata":{}},{"cell_type":"markdown","source":" # Peak Signal-to-Noise Ratio (PSNR):","metadata":{}},{"cell_type":"code","source":"print(f\"PSNR: {psnr} dB\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Interpretation:\n\n    PSNR measures the quality of the reconstructed image. A higher PSNR indicates a better reconstruction. Itâ€™s the ratio between the maximum possible power of an image and the power of the noise.\n    Scale: Typical PSNR values range from 20 to 40 for image reconstruction tasks, with higher values being better. PSNR > 30 dB is generally considered a good result.","metadata":{}},{"cell_type":"markdown","source":"# Structural Similarity Index (SSIM):","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nssim_map = tf.image.ssim(Y_test[0], predictions[0], max_val=1.0)\nprint(f\"SSIM: {ssim_map}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T10:05:51.162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Interpretation:\n\n    SSIM measures the perceived quality of the image by comparing structural information between the original and reconstructed images. It ranges from -1 to 1, where 1 indicates perfect similarity.\n    Scale: A value closer to 1 indicates better structural similarity between the images. Typically, SSIM > 0.9 is considered good.","metadata":{}}]}